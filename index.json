[{"content":" We\u0026rsquo;ll be using yml/yaml format for all examples down below, it is recommend to use yaml over toml as it is easier to read. You can find any YML to TOML converters if needed. Getting Started üöÄ Follow Hugo Docs\u0026rsquo;s - Quick Start guide to install . (Make sure you install Hugo \u0026gt;= v0.112.4)\nCreate a new site\nhugo new site MyFreshWebsite --format yaml # replace MyFreshWebsite with name of your website Note:\nOlder versions of Hugo may not support --format yaml Read more here about Hugo Docs\u0026rsquo;s - hugo new site command After you have created a new site, follow the below steps to add PaperMod\nInstalling/Updating PaperMod Themes reside in MyFreshWebsite/themes directory. PaperMod will be installed in MyFreshWebsite/themes/PaperMod Expand Method 1 - Git Clone INSTALL : Inside the folder of your Hugo site MyFreshWebsite, run:\ngit clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 You may use --branch v7.0 to end of above command if you want to stick to specific release.\nUPDATE: Inside the folder of your Hugo site MyFreshWebsite, run:\ncd themes/PaperMod git pull Expand Method 2 - Git Submodule (recomended) INSTALL : Inside the folder of your Hugo site MyFreshWebsite, run:\ngit submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive # needed when you reclone your repo (submodules may not get cloned automatically) You may use --branch v7.0 to end of above command if you want to stick to specific release. Read more about git submodules here.\nUPDATE: Inside the folder of your Hugo site MyFreshWebsite, run:\ngit submodule update --remote --merge Expand Method 3 - Download an unzip Download PaperMod source as Zip from Github Releases and extract in your themes directory at MyFreshWebsite/themes/PaperMod\nDirect Links:\nMaster Branch (Latest) v7.0 v6.0 v5.0 v4.0 v3.0 v2.0 v1.0 Expand Method 4 - Hugo module INSTALL :\nInstall Go programming language in your operating system.\nIntialize your own hugo mod\nhugo mod init YOUR_OWN_GIT_REPOSITORY Add PaperMod in your config.yml file 1 2 3 module: imports: - path: github.com/adityatelange/hugo-PaperMod UPDATE:\nhugo mod get -u Read more : Hugo Docs\u0026rsquo;s - HUGO MODULES\nFinally set theme as PaperMod in your site config In config.yml add:\n1 theme: [\u0026#34;PaperMod\u0026#34;] Next up - Customizing PaperMod to suit your preferences. Your site will be blank after you set up for the very first time. You may go through this website\u0026rsquo;s source code - PaperMod\u0026rsquo;s exampleSite\u0026rsquo;s souce Scroll below this page where you will find more specific details about each section. Kindly go through all of the pages below to know how to configure PaperMod. Support ü´∂ Star üåü PaperMod\u0026rsquo;s Github repository. Help spread the word about PaperMod by sharing it on social media and recommending it to your friends. üó£Ô∏è You can also sponsor üèÖ on Github Sponsors / Ko-Fi. Videos featuring PaperMod You can go through few videos which are available on YouTube for getting to know the creator\u0026rsquo;s thoughts as well as the setup process.\n‚ñ∂Ô∏è https://youtube.com/playlist?list=PLeiDFxcsdhUrzkK5Jg9IZyiTsIMvXxKZP\nQuick Links Papermod - Features Papermod - FAQs Papermod - Variables Papermod - Icons ChangeLog Sample config.yml Example Site Structure is present here: exampleSite\nUse appropriately\nbaseURL: \u0026#34;https://examplesite.com/\u0026#34; title: ExampleSite paginate: 5 theme: PaperMod enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false googleAnalytics: UA-123-45 minify: disableXML: true minifyOutput: true params: env: production # to enable google analytics, opengraph, twitter-cards and schema. title: ExampleSite description: \u0026#34;ExampleSite description\u0026#34; keywords: [Blog, Portfolio, PaperMod] author: Me # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors images: [\u0026#34;\u0026lt;link or path of image for opengraph, twitter-cards\u0026gt;\u0026#34;] DateFormat: \u0026#34;January 2, 2006\u0026#34; defaultTheme: auto # dark, light disableThemeToggle: false ShowReadingTime: true ShowShareButtons: true ShowPostNavLinks: true ShowBreadCrumbs: true ShowCodeCopyButtons: false ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true disableSpecial1stPost: false disableScrollToTop: false comments: false hidemeta: false hideSummary: false showtoc: false tocopen: false assets: # disableHLJS: true # to disable highlight.js # disableFingerprinting: true favicon: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; favicon16x16: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; favicon32x32: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; apple_touch_icon: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; safari_pinned_tab: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; label: text: \u0026#34;Home\u0026#34; icon: /apple-touch-icon.png iconHeight: 35 # profile-mode profileMode: enabled: false # needs to be explicitly set title: ExampleSite subtitle: \u0026#34;This is subtitle\u0026#34; imageUrl: \u0026#34;\u0026lt;img location\u0026gt;\u0026#34; imageWidth: 120 imageHeight: 120 imageTitle: my image buttons: - name: Posts url: posts - name: Tags url: tags # home-info mode homeInfoParams: Title: \u0026#34;Hi there \\U0001F44B\u0026#34; Content: Welcome to my blog socialIcons: - name: x url: \u0026#34;https://x.com/\u0026#34; - name: stackoverflow url: \u0026#34;https://stackoverflow.com\u0026#34; - name: github url: \u0026#34;https://github.com/\u0026#34; analytics: google: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; bing: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; yandex: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; cover: hidden: true # hide everywhere but not in structured data hiddenInList: true # hide on list pages and home hiddenInSingle: true # hide on single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link # for search # https://fusejs.io/api/options.html fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 limit: 10 # refer: https://www.fusejs.io/api/methods.html#search keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] menu: main: - identifier: categories name: categories url: /categories/ weight: 10 - identifier: tags name: tags url: /tags/ weight: 20 - identifier: example name: example.org url: https://example.org weight: 30 # Read: https://github.com/adityatelange/hugo-PaperMod/wiki/FAQs#using-hugos-syntax-highlighter-chroma pygmentsUseClasses: true markup: highlight: noClasses: false # anchorLineNos: true # codeFences: true # guessSyntax: true # lineNos: true # style: monokai Sample Page.md --- title: \u0026#34;My 1st post\u0026#34; date: 2020-09-15T11:30:03+00:00 # weight: 1 # aliases: [\u0026#34;/first\u0026#34;] tags: [\u0026#34;first\u0026#34;] author: \u0026#34;Me\u0026#34; # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors showToc: true TocOpen: false draft: false hidemeta: false comments: false description: \u0026#34;Desc Text.\u0026#34; canonicalURL: \u0026#34;https://canonical.url/to/page\u0026#34; disableHLJS: true # to disable highlightjs disableShare: false disableHLJS: false hideSummary: false searchHidden: true ShowReadingTime: true ShowBreadCrumbs: true ShowPostNavLinks: true ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true cover: image: \u0026#34;\u0026lt;image path/url\u0026gt;\u0026#34; # image path/url alt: \u0026#34;\u0026lt;alt text\u0026gt;\u0026#34; # alt text caption: \u0026#34;\u0026lt;text\u0026gt;\u0026#34; # display caption under cover relative: false # when using page bundles set this to true hidden: true # only hide on current single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link --- You can use it by creating archetypes/post.md\nhugo new --kind post \u0026lt;name\u0026gt; ","permalink":"https://XIAOLAN-design.github.io/posts/papermod/papermod-installation/","summary":"Read aboout Install and Update instructions and sampled configuration templates","title":"Install / Update PaperMod"},{"content":"Intro We\u0026rsquo;ll be using yml/yaml format for all examples down below, I recommend using yml over toml as it is easier to read.\nYou can find any YML to TOML converters if necessary.\nAssets (js/css) The following is enabled by default\nminification - makes the assets size smallest as possible. bundling - bundles all the styles in one single asset fingerprint/intergity check. Default Theme light/dark/auto 1 2 3 4 params: # defaultTheme: light # defaultTheme: dark defaultTheme: auto # to switch between dark or light according to browser theme Theme Switch Toggle (enabled by default) Shows icon besides title of page to change theme\nTo disable it :\n1 disableThemeToggle: true You can refer following table for better understanding\u0026hellip;\ndefaultTheme disableThemeToggle checks local storage? checks system theme? Info auto true No Yes only system theme false Yes (if not-\u0026gt;2) Yes (2) switch present dark true No No force dark only false Yes No switch present light true No No force light only false Yes No switch present Archives Layout Create a page with archive.md in content directory with following content\n. ‚îú‚îÄ‚îÄ config.yml ‚îú‚îÄ‚îÄ content/ ‚îÇ ‚îú‚îÄ‚îÄ archives.md \u0026lt;--- Create archive.md here ‚îÇ ‚îî‚îÄ‚îÄ posts/ ‚îú‚îÄ‚îÄ static/ ‚îî‚îÄ‚îÄ themes/ ‚îî‚îÄ‚îÄ PaperMod/ and add the following to it\n--- title: \u0026#34;Archive\u0026#34; layout: \u0026#34;archives\u0026#34; url: \u0026#34;/archives/\u0026#34; summary: archives --- Note: Archives Layout does not support Multilingual Month Translations.\nex: archives.md\nRegular Mode (default-mode) Home-Info Mode Use 1st entry as some Information\nadd following to config file\nparams: homeInfoParams: Title: Hi there wave Content: Can be Info, links, about... socialIcons: # optional - name: \u0026#34;\u0026lt;platform\u0026gt;\u0026#34; url: \u0026#34;\u0026lt;link\u0026gt;\u0026#34; - name: \u0026#34;\u0026lt;platform 2\u0026gt;\u0026#34; url: \u0026#34;\u0026lt;link2\u0026gt;\u0026#34; Profile Mode Shows Index/Home page as Full Page with Social Links and Image\nadd following to config file\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 params: profileMode: enabled: true title: \u0026#34;\u0026lt;Title\u0026gt;\u0026#34; # optional default will be site title subtitle: \u0026#34;This is subtitle\u0026#34; imageUrl: \u0026#34;\u0026lt;image link\u0026gt;\u0026#34; # optional imageTitle: \u0026#34;\u0026lt;title of image as alt\u0026gt;\u0026#34; # optional imageWidth: 120 # custom size imageHeight: 120 # custom size buttons: - name: Archive url: \u0026#34;/archive\u0026#34; - name: Github url: \u0026#34;https://github.com/\u0026#34; socialIcons: # optional - name: \u0026#34;\u0026lt;platform\u0026gt;\u0026#34; url: \u0026#34;\u0026lt;link\u0026gt;\u0026#34; - name: \u0026#34;\u0026lt;platform 2\u0026gt;\u0026#34; url: \u0026#34;\u0026lt;link2\u0026gt;\u0026#34; Search Page PaperMod uses Fuse.js Basic for search functionality\nAdd the following to site config, config.yml\n1 2 3 4 5 outputs: home: - HTML - RSS - JSON # necessary for search Create a page with search.md in content directory with following content\n1 2 3 4 5 6 7 8 --- title: \u0026#34;Search\u0026#34; # in any language you want layout: \u0026#34;search\u0026#34; # necessary for search # url: \u0026#34;/archive\u0026#34; # description: \u0026#34;Description for Search\u0026#34; summary: \u0026#34;search\u0026#34; placeholder: \u0026#34;placeholder text in search input box\u0026#34; --- To hide a particular page from being searched, add it in post\u0026rsquo;s frontmatter\n1 searchHidden: true ex: search.md\nSearch Page also has Key bindings:\nArrow keys to move up/down the list Enter key (return) or Right Arrow key to go to the highlighted page Escape key to clear searchbox and results For Multilingual use search.\u0026lt;lang\u0026gt;.md ex. search.es.md.\nNote: Search will work only on current language, user is currently on !\nCustomizing Fusejs Options\nRefer https://fusejs.io/api/options.html for Options, Add those as shown below.\n1 2 3 4 5 6 7 8 9 10 params: fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 # limit: 10 # refer: https://www.fusejs.io/api/methods.html#search keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] Draft Page indication adds [draft] mark to indicate draft pages.\nPost Cover Image In post\u0026rsquo;s page-variables add :\n1 2 3 4 5 6 7 cover: image: \u0026#34;\u0026lt;image path/url\u0026gt;\u0026#34; # can also paste direct link from external site # ex. https://i.ibb.co/K0HVPBd/paper-mod-profilemode.png alt: \u0026#34;\u0026lt;alt text\u0026gt;\u0026#34; caption: \u0026#34;\u0026lt;text\u0026gt;\u0026#34; relative: false # To use relative path for cover image, used in hugo Page-bundles When you include images in the Page Bundle, multiple sizes of the image will automatically be provided using the HTML5 srcset field.\nTo reduce generation time and size of the site, you can disable this feature using\n1 2 3 params: cover: responsiveImages: false To enable hyperlinks to the full image size on post pages, use\n1 2 3 params: cover: linkFullImages: true Share Buttons on post Displays Share Buttons at Bottom of each post\nto show share buttons add\nparams: ShowShareButtons: true Show post reading time Displays Reading Time (the estimated time, in minutes, it takes to read the content.)\nTo show reading time add\nParams: ShowReadingTime: true Show Table of Contents (Toc) on blog post Displays ToC on blog-pages\nTo show ToC add following to page-variables\nShowToc: true To keep Toc Open by default on a post add following to page-variables:\nTocOpen: true BreadCrumb Navigation Adds BreadCrumb Navigation above Post\u0026rsquo;s Title to show subsections and Navigation to Home\nparams: ShowBreadCrumbs: true Can be diabled for particular page\u0026rsquo;s front-matter\n--- ShowBreadCrumbs: false --- Edit Link for Posts Add a button to suggest changes by using the file path of the post to link to a edit destination.\nFor site config use:\nParams: editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link Can be modified for individual pages\n--- editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link --- The example above would yield the following link for the post file posts/post-name.md: https://github.com/\u0026lt;path_to_repo\u0026gt;/content/posts/post-name.md\nParameter Required Default Value editPost.URL true - editPost.appendFilePath false false editPost.Text false Edit Since the link generated is a regular HTML anchor tag \u0026lt;a href=...\u0026gt;, you can also use other URL schemas like mailto://, e.g. URL: \u0026quot;mailto://mail@example.com?subject=Suggesting changes for \u0026quot;\nOther Posts suggestion below a post Adds a Previous / Next post suggestion under a single post\nparams: ShowPostNavLinks: true Code Copy Button Adds a copy button in code block to copy the code it contains\nparams: ShowCodeCopyButtons: true Multiple Authors To Use multiple authors for a post, in post-variables:\n--- author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] --- To use Multiple Authors Site-wide, in config.yml:\nparams: author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] Comments to add comments, create a html file\nlayouts/partials/comments.html\nand paste code provided by your comments provider\nalso in config add this\nparams: comments: true read more about this hugo-comments\nAccessKeys c - ToC Open/Close g - Go To Top h - Home (according to current lang) t - Theme toggle / - Jumps to search page if in menu What\u0026rsquo;s AccessKeys ?\nEnhanced SEO Enabled only when env: production\nRich Results/Snippets Support Twitter Cards Support The Twitter Cards metadata, except twitter:image should not require additional configuration, since it is generated from metadata that you should already have (for instance the page title and description). The twitter:image uses the Post Cover Image, if present. In the absence of a cover images, the first image from the images frontmatter (a list) is used. images: - image_01.png - image_02.png Finally, if neither of those are provided, twitter:image comes from the first Page Bundle image with feature in the name, with a fallback to the first image with cover or thumbnail in the name. OpenGraph support The OpenGraph metadata, except og:image should not require additional configuration, since it is generated from metadata that you should already have (for instance the page title and description). The og:image uses the Post Cover Image, if present. In the absence of a cover images, the first image from the images frontmatter (a list) is used. images: - image_01.png - image_02.png Finally, if neither of those are provided, og:image comes from the first Page Bundle image with feature in the name, with a fallback to the first image with cover or thumbnail in the name. For pages, you can also add audio (using frontmatter audio: filename.ext) and/or videos. videos: - filename01.mov - filename02.avi Multilingual Support Misc Scroll-Bar themed (by default) Smooth Scroll between in-page links (by default) Scroll-to-Top Button (by default) Displays a Scroll-to-Top button in right-bottom corner Google Analytics integration Syntax highlighting RSS feeds ","permalink":"https://XIAOLAN-design.github.io/posts/papermod/papermod-features/","summary":"Learn About All Features in PaperMod","title":"Features / Mods"},{"content":"This Blog is devided into 2 parts. First part is the review of discussion of Emergence. You can just skip part 2 if you are not interested in theoretical staff. Second part illustrates the theory of emergence of complex skills in language model in a way people can understand easily, which is based on (Sanjeev et al. 2023) and the modified version of the paper Prof Arora (the author of the paper) sent me. Many thanks for the modified version of the paper and his comments on this blog.\nThe motivation for this blog is to make the relating theory easy for people to understand, so I put more illustration work on this blog but not omit some important theoretical staff. All of the formal definitions, assumptions and theorems in this paper are from those two resources above and other relating papers that I will refer later.\nThe first contribution is that I explained a few concepts in the paper in more detail. For example, in the 1st part of the blog, I gave a full review about what is emergence and what is the emergence of complex skills which the authors didn\u0026rsquo;t explain well. In the 2nd part, I wrote down the intuitions of the main theorems and how they were connected to the LLM which was not covered in the original paper.\nThe second contribution is that I added the proofs of a few statements which the author skipped in his paper (see Appendix 3.1, 3.2). I also reconstructed the proof of some theorems in a simpler way(see Appendix 3.3).\nConsidering the paper (Sanjeev et al. 2023) is not finalized, thus this blog cannot be yet finalized,too. I will keep it updated once I recieved more information from Professor Arora.\nPart 1: Discussion of Emergence The exsitance of emergence has been a hated topic recently. The debate mainly comes from different definition of emergence, thus we will review all of the definitions in the following.\n1 Real Defintion of Emergence Emergence is a popular phenomena in domains such as physics, biology, etc. The general definition of emergence, adapted from Steinhardt (2022) and rooted in a 1972 essay called ‚ÄúMore Is Different‚Äù by Nobel prize-winning physicist Philip Anderson (Anderson, 1972):\nQuantitative changes in a system result in qualitative changes in behavior. The emergent abilities of large language models also attract lots of attention recentlly. However, the official definition of Emergence of Language Models remains in hated discussion.\n1.1 Outdated Definition of Emergence (Wei et al. 2022) first introduced emergent abilities of large language models, it was defined as following:\nSharpness, transitioning seemingly instantaneously from not present to present. Unpredictability, transitioning at seemingly unforeseeable model scales Figure 1: see (Wei et al. 2022). Eight examples of emergence in the few-shot prompting setting. Each point is a separate model. The ability to perform a task via few-shot prompting is emergent when a language model achieves random performance until a certain scale, after which performance significantly increases to well-above random.\nFigure 2:see (Wei et al. 2022). Analogous figure with number of model parameters instead of training flops in figure 1. Obviously it shows Figure 1 and figure 2 have the exactly same performance, which means models that used more training compute also typically have more parameters‚Äîhence.\n1.2 Is Emergence a Mirage? However, there was a time that people both from industry and academia doubted the existance of Emergent Abilities for language models. They argued those models actually do not show Sharpness,transitioning, unpredictability performance improvement when the model is scaled up, thus do not have Emergent Abilities.\nThere is no doubt the model gets better performance with more number of parameters and training flops.\nBut is the better performance just the sum of small improvement of each scaling up stage or is it greater than sum of those? Whether the sharp transition of the performance curve come from the choice of evaluation of metrics? see more in (Emergent Abilities Are a Mirage) Figure 3:see (Schaeffer et al. 2023). Claimed emergent abilities evaporate upon changing the metric. Left to Right: Mathematical Model, 2-Integer 2-Digit Multiplication Task, 2-Integer 4-Digit Addition Task. Top: When performance is measured by a nonlinear metric (e.g., Accuracy), the InstructGPT/GPT-3 [3, 24] family‚Äôs performance appears sharp and unpredictable on longer target lengths. Bottom: When performance is instead measured by a linear metric (e.g., Token Edit Distance), the family exhibits smooth, predictable performance improvements.\n(Schaeffer et al. 2023) conducted lots of experiments to prove that the sharpness, transitioning and unpredictability performance improvment of large language model is a creation of chosen metrics. Typically, linear or continuous metrics produce smooth,continuous,predictable changes in model performance, while a nonlinear or discontinuous metric can distort the model family‚Äôs performance from continuous, predictable changes to appear sharp, unpredictable.\nHowever, the explanation of metric choice above cannot be the evidence to deny the exsitance of emergent abilities. Since eventhough the performance does not show sharp transitioning, the improvement is still unpredictable to some extent.\nAs picture above shows, the performance improvement does not follow a deterministic function of scaling , which means it is unpredictable.\nAlso, the increasing rate of performance improvement is getting bigger and bigger, which means the performance improvement is greater than sum of small improvement of each scaling up stage, which obviously support that new skills will be emergent in large models.\n1.3 Latest Defition of Emergence Slow emergence is more widely accepted recently, meaning that the performance curve does not have to appear as a sharp transition when the model is scaled up. The sharp transition might be due to the final step metric evaluation method we used for meausring model performance. That is: solving one task reguires several steps, the models performance is actually incresing in each scaling step. but the evaluation metric just count for final step, which will result in the sharp transition of performance curve.\nEmergence is firstly introduced as Slow Emergence by (Sanjeev et al. 2023), which is defined as follows:\nAs number of sample sizes and number of parameters increase together then the model‚Äôs performance on a broad range of language tasks improves in a correlated way. I personally do not totally agree with the definition above. I agree that the performance curve does not have to be sharp transition part, but the performance increasement should be 1+1\u0026gt;2, which means the better performance is not the sum of small improvement of each scaling up stage, it should be greater than sum of those. however, the definition above does not illustrate 1+1\u0026gt;2.\nIn the paper by (Sanjeev et al. 2023), emergence of simple skills is defined as slow emergence above. Emergence of complex skills is defined as something new (those were not seen during training means new things) emerge\nThere are further discussion about emergence, (Lu et al. 2023) firstly mentioned Emergent abilities are In-context Learning. At the moment I will not extend this part.\nPart 2: theory of emergence of complex skills in language model I will first give a intuition about why complex skills emerge in language models. We have a text corpus, the the size of the training corpus is almost same with number of simple skills. We define complex skill as k\u0026rsquo;-tuple skill, which is composed of k\u0026rsquo; simple skills (you can find details about definitions about corpus and skills below).\nAssume we have 1000 training size and 1000 simple skills, 5 simple skills(k'=5) can compose a complex skill, thus we have \\(1000^{5}\\) =\\(10^{15}\\)combinations of complex skills. Since the training size is 1000, it can only see 1000/\\(10^{15}\\)=1/\\(10^{12}\\) proportion of complex skills during training(learning). We know that the model can only learn seen skills, if the model can only see 1/\\(10^{12}\\) proportion of complex skills during training, then it displays competence on complex skills is at most 1/\\(10^{12}\\) proportion. Thus if the model can display competency on even 10% proportion, which means the model emerges 10%-1/\\(10^{12}\\) complex skills despite it has never learned(seen during training) those complex skills. Thus, in order to show the emergence of complex skills, we need to show the competence of complex skills is at least not too small (bigger than e.g. 1/\\(10^{12}\\)), which means lots of new complex skills that are not seen during training process emerge. Here we introduce competence of simple skills to help us to quantify the competence of complex skills. If we can show that the competence of complex skill after scaling is at least the same level of competence of simple skills, which means the competence of complex skills is at least not too small since the competence of simple skills is not too small(since each simple skill will be seen thus be learned during training, so the competence of simple skills are not small).\nSo in part 2, the main goal is to quantify the competence of skills, then connect competence, scaling law and loss together to show that the competence of complex skill after scaling is at least the same level of competence of simple skills. Thus with the competence of complex skill bigger than the proportion of complex skills will be seen during training, we can conclude that the model can emerge new complex skills that are not seen during training when scaling up.\nThe structure of part 2 is as follows: we firstly introduce some basic knowledge for analyzing emergence in 2.1, 2.2 and 2.3. Then based on these, in 2.4.1 we analyze slow emergence(recall real definition of emergence in part 1) in language model from statistical view, which (Sanjeev et al. 2023) is as follows:\nAs the model‚Äôs excess cross entropy goes down due to scaling, the model‚Äôs performance improves.\nThen in 2.4.2 we continue introducing how complex skills(k\u0026rsquo; skills) emerge due to scaling, the key result as follows shows that if we scale up the model, the performance we get for complex skills is equal to performance we get for simple skills without scaling (Sanjeev et al. 2023):\nIf models of size S have a certain success rate (compentence) at solving tasks that require k\u0026rsquo; skills, then Scaling Laws imply that scaling up models (i.e. to size 10S leads to factor 2 reduction in loss) will give them the same success rate on tasks that involve combining 2k\u0026rsquo; skills as what we get on k\u0026rsquo; skills.\n2.1 Skills of Language Moldels First, we will clarify some terminologies required for this theory. There is no official definition for Skills of Language Model. Generally speaking, for those words generated by humanbeings or by neural network models, some skills are applied. (Sanjeev et al. 2023) develped a new theory as follows to describe skills as those for generating text pieces.\n2.1.1 Text Pieces Definition of Text Piece: (Sanjeev et al. 2023)\nThere is a measure \\(¬µ_{2}()\\) on these text-pieces, with \\(¬µ_{2}(t)\\) denoting the measure of text-piece t. The usual cross-entropy loss is computed by weighting text-pieces with respect to this measure. Assume we choose one piece of news from NewYork Times as our corpus, text piece should be thought of as having a size between a paragraph to a few pages, drawn from a longer corpus. Equivalently speaking, we can divide the news into text-pieces, each text pieces consitsts \\(C\\) tokens. We have train process and test process, thus corpus is devided as train corpus and test corpus, respectively. We will see in scling law introduced later that only test corpus is considered in this theory. --\u003e 2.1.2 Skills Definition of skill Graph: (Sanjeev et al. 2023)\nA skill graph is a bipartite graph \\((V_{1}, V_{2}, E)\\) where nodes in \\(V_{2}\\) correspond to skills, there is a measure \\(¬µ_{1}()\\) on these skills, with \\(¬µ_{1}(s)\\) denoting the measure of skill \\(s\\). Figure 4:see (Sanjeev et al. 2023). Each text piece can be generated by applying several skills.\nFor each text piece, skills are required for generating it. For the sentence above, What kinds of skills could it apply to generate the text pieces? You will have the answer after getting to know what do skills mean. We have example of skills as follows.\nNamed Entity Recognition (NER) pronoun disambiguation Sentiment Analysis Anaphora Resolution Logical Inference World Knowledge Barack Obama was born in Hawaii. He served as the 44th President of the United States. For the sentence above, Barack Obama is a name, Hawaii is a location, and the United States is a country, all of which can be determined by the skill Named Entity Recognition (NER). And by applying pronoun disambiguation can we infer He means Barack Obama.\nI love waiting in long lines. It makes me want to cut myself The sentence above is bit hard since it is an ironical expression, the word love here is not really love, it means hate instead, which requires the skill entiment analysis to analyze the sentiment of this expression is negative by the negative verb cut myself.\nThe CEO of the company made an announcement. It surprised everyone, so they get together to drink in the bar. It in the sentence means the thing he CEO of the company made an announcement, which requires the skill Anaphora Resolution. Since everyone is amazed by the god news, so we can infer everyone is happy, then they will celebrate in some place like in the bar, which requires the skill Logical Inference.\nThough I just list several skills for one sentence, the truth is when we generate the sentence, we have to apply almost every skills we want. the skills above are those named by human. However, there are more skills that are not found or named by human beings. For instance, those are lots of skills used in large language models like GPT, we human however cannot know each specific skills the model applies to generate text.\nAfter getting to know those examples above, a quick quiz for you: what kinds of skills could it apply to generate the following text pieces?\nThe city councilmen refused the demonstrators a permit because they feared violence.\n2.2 Cross Entropy Loss to Scaling Law Since we want to connect scaling law with skills, we can choose cross entropy as our bridge. We first find connection between cross entropy and scaling law, then we find connection between cross entropy and skills, thus we can find connection between scaling law and skills. So in this part, we are gonna first intriduce cross entropy then scaling law, then we will find connection between them.\n2.2.1 Cross Enrtopy For word generation, we always have a sequence of previous words to generate the next word, which is also named as next word prediction. Similar to other predicting process, language generating process also has 2 probability. One is the probability of the next predicted word of the current model, the other is the ground-truth probability (i.e. humans' choice), which can be understood as the true label for next word prediction process. Given the previous words \\(w_1 w_2 \\ldots w_i\\), the probability distribution of the model itself \\(q_{i}\\left(w\\right)\\), which can be defined as \\(\\operatorname{q}_i\\left[w \\mid w_1 w_2 \\ldots w_i\\right]\\). The ground truth distribution \\(p_{i}\\left(w\\right)\\) for generating the next \\((i + 1)th\\) word \\(w\\), which is defined as \\(\\operatorname{p}_i\\left[w \\mid w_1 w_2 \\ldots w_i\\right]\\). $$ p_{i}\\left(w\\right)\\ = \\begin{cases} 1, \u0026 \\text{if } w = w_{i+1} \\\\ 0. \u0026 \\text{if} w\\not =w_{i+1} \\end{cases}\\quad (1) $$ We take a particular interest in the difference between predicted word and the ground-truth word. The Cross Entropy can get a measure of dissimilarity between \\(p_{i}\\left(w\\right)\\) and \\(q_{i}\\left(w\\right)\\).Thus we introduce cross-entropy loss of the model on one word is: $$ \\sum_w p_i(w) \\log \\frac{1}{q_i(w)} \\quad(\\text {Cross Entropy of the word}) \\quad (2) $$ We have many attempts to make predictions, only one attempt is correct. That means only one attempt with ground-truth probability of 1, others are 0. So we sum all of the attempts (w) together to get the cross entropy on the ((i+1)th) word.\n$$ \\sum_w p_i(w_{i+1}) \\log \\frac{1}{q_i(w_{i+1})} = \\log \\frac{1}{q_i(w_{i+1})}\\quad (3) $$\nIt is easy to be confused between the Cross Entropy of the model and Cross Entropy of the word. Cross Entropy of the model is the total cross entropy of each word in the test corpus. After getting cross entropy of the word, we need to sum up all the cross entropy of the word to get the cross entropy of the model M.\n$$ \\ell(M)=\\sum_{i=1}^{N} \\log \\frac{1}{q_i(w_{i+1})} =-\\sum_{i=1}^{N} \\log q_i(w_{i+1}) \\quad (4) $$ which can be defined as follows: $$ =-\\sum_i \\log \\underset{M}{\\operatorname{Pr}}\\left[w_{i+1} \\mid w_1 w_2 \\ldots w_i\\right] \\quad \\text { (Cross Entropy of the Model) } \\quad (5) $$\nWe are also interested in the inherent property of language, we know human can have lots of choices for making the next word. Entropy can be used to describe the \"uncertainty\" inherent to the variable's possible outcomes. Thus we use Entropy to descibe inherent uncertainty of one word prediction: $$ \\sum_w p_i(w) \\log \\frac{1}{p_i(w)} \\quad(\\text {Entropy}) \\quad (6) $$ We also introduce KL divergence, which quantifies the information loss when the predicted distribution \\(q_{i}\\left(w\\right)\\) is used to approximate the true distribution \\(p_{i}\\left(w\\right)\\). it is also sometimes called excess entropy, is non-negative and defined as: $$ K L\\left(p_i \\| q_i\\right)=\\sum_w p_i(w) \\log \\frac{p_i(w)}{q_i(w)} \\quad(\\text {Excess Entropy}) \\quad (7) $$ Then we can find an intersting relationship on a per-word basis from equation\\((1),(2)\\) and \\((3)\\): $$ \\text { Corss Entropy }=\\text { Entropy }+ \\text { Excess Entropy} \\quad (8) $$ 2.2.2 Scaling Law Recall in part 1 we mentioned the real definition of Emergence:\nAs number of sample sizes and number of parameters increase together then the model‚Äôs performance on a broad range of language tasks improves in a correlated way.\nThe definition above reflects Scaling law, which describes how test cross entropy loss on test experiments scales with number of model parameters (N) and size of the dataset (D) Researchers have conducted lots of expiriments to derive Scaling law, Hoffmann et al. [2022] derived the scaling law is as follows:\n$$ L(N, D) = A + \\frac{B}{N^{0.34}} + \\frac{C}{D^{0.28}} \\quad A=1.61 \\quad B=406.4 \\quad C=410.7 \\quad (9) $$ 2.2.3 Understanding the Scaling Law in terms of excess entropy From [(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936), if we compare equation \\((9)\\) with \\((8)\\), the \\(A\\) term of \\((9)\\) captures the entropy of language. The lowest cross-entropy loss is entropy, that is term \\(A\\) for large corpora. The second and third terms of \\((9)\\) capture excess entropy, and they decrease polynomially with \\(N\\) and \\(D\\). For example when \\(D\\) is increased by a factor of 10 it reduces by roughly \\(10^{0.28} \\approx 2\\). This partly explains well the choice of metric evaluation could change the oerformance curve from sharp transition into slow increasement. As picture below shows: If we replace our loss metric from error rate as cross-entropy loss. We can observe the sharp transition of the perfromance curve disappear.\nThat is because the term \\(A\\) is a constant term, while the improvement of performance is brought by the sencond and third term. Thus, when there is some improvement for the sencond and third term, after we plus the first constant term \\(A\\), the curve will not show obvious improvement. Thus given the fact that excess cross entropy is the real driver in language generating process, it seems that we just need to consider **Excess Cross Entrioy** in our theory. Figure 5:see (Schaeffer et al. 2023). Adjacent plots for error rate, cross-entropy loss, and log probabilities of correct and incorrect responses on three classification tasks on BIG-Bench that we consider to demonstrate emergent abilities. Logical arguments only has 32 samples, which may contribute to noise. Error rate is (1 - accuracy).\n2.3 Skills modelling Skill modelling is a process of finding relationship between excess cross entropy and compentence on the skills. First we will make some assumptions, then we can quantify the model‚Äôs competence on particular skill.\n2.3.1 Mixing Assumption Each text-piece \\(t\\) can be generated by picking \\(k-tuple\\) of skills iid from measure \\(¬µ_{1}()\\) sing an unknown process, which assigns probability \\(¬µ_{2}(t)\\). Figure 6:Created by Xiaolan Liu. Skill graph where each text-piece t can be generated by picking k‚àítuple of skills .\nIt is assumed that we have a set of skills, which is pretty large and bit smaller than the number of text-pieces text pieces. Then text pieces are generated by picking random k-tuples of skills. l\n2.3.2 Scaling law Assumption The theory will assume scaling laws such as (9), thus it can reason directly about the model‚Äôs behavior on the test distribution We know the training corpus is quite large, which makes it hard to model our theory in training process. Luckily, the scaling laws help us avoid reasoning about training and generalization. The scaling law is based on test cross entropy loss, thus our theory does not need to refer to training cross entropy loss.\n2.3.3 Cloze Sufficiency Assumption To test the \\(k\\) underlying skills in \\(t\\), adds cloze prompts to \\(t\\) via an unknown process. The pre-trained model‚Äôs average prediction loss on Cloze questions (where the average is taken over the distribution of text pieces) closely tracks the excess cross-entropy of the model on classical next-word prediction.\nTheorem 1 in Appendix 3.1 justifies the exsitence of this assumption. You can find detailed math proof in Appendix.\nThe excess cross-entropy of the model on classical next-word prediction is not easy to calculate. Here we need to consider a equivalent method to get excess cross entropy loss, which is cloze questions approach. That means we add multiple choice question answering on text pieces to test the language model\u0026rsquo;s ability of understanding. Think the example below mentioned in (Sanjeev et al. 2023)\nThe city councilmen refused the demonstrators a permit because they feared violence.\nHere the pronoun they is ambiguous‚Äî grammar rules allow it to refer to either demonstrators or city councilmen. To test the model‚Äôs understanding of they in this sentence, we can append a prompt:\nQ. Who feared violence? A. city councilmen B.demonstrators\nAssume there are \\( N \\) text pieces in our model, we add \\( Q_i \\) cloze questions on this text piece above. The prediction loss on the text-piece above is the cross-entropy loss on predicting the answers to the cloze questions in it. The average prediction loss over all text-pieces is computed with respect to the measure \\(¬µ_{2}()\\), if we assume measure \\(¬µ_{2}()\\) is uniform. Then we have the average prediction loss over all text-pieces, noted as \\( \\delta \\), is defined as follows: \\[ \\delta = \\frac{1}{N} \\sum_{i=1}^N \\text{cross entropy on text piece } i \\] \\[ = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\text{cross entropy on cloze question } z_j^{(i)} \\] 2.3.4 compentence on the skills By cloze sufficiency assumption above, we replace model‚Äôs average prediction loss on Cloze questions as excess cross-entropy of the model on classical next-word prediction. Now we need to connect model‚Äôs average prediction loss on Cloze questions and compentence on the skills.\nFor a skill s, the competence of a model defined in the modified version of the paper Sanjeev sent to me: it is the expectation of the following random variable: randomly sample a text-piece containing that skill (this sampling uses the measure \\(¬µ_{2}(¬∑)\\) on text-pieces) and measure the model‚Äôs success rate (1 - \\( \\delta \\)) at answering cloze questions in that text piece. We similarly define Competence on a tuple of skills (s1,s2,...,). 2.4 Analysis of Emergence Now we have set up a framework for modeling skills and connecting skills to the cross-entropy loss of the model, we firstly derive a mathematical formula for connections between loss, simple skills and scaling factor, then we have result: As the model‚Äôs excess cross entropy goes down due to scaling, the model‚Äôs performance on cloze tasks improves.\nAfter that, we derive a mathematical formula for connections between loss, complex skills and scaling factor, then we have result: the performance curve inferred by our method for k\u0026rsquo;-tuples of skills after scaling up is identical to the curve inferred for individual skills without scaling up.\n2.4.1 Emergence for Simple Skills Figure 7:Created by Xiaolan Liu. Skill graph where Y is the subset of such text pieces where the model makes mistakes on cloze questions.\n[(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936) mentioned how do we set up the theory in his paper: Let Y be the subset of such text pieces where the model makes mistakes on cloze questions. Let‚Äôs say the model makes a mistake on a text-piece if the total prediction loss on all the cloze questions of that text-piece is at least 1/2. Say Y contains \\(\\theta\\) fraction of text-pieces. Note that we cannot confuse \\(\\theta\\) with the average excess cross-entropy loss for those text-pieces. We have N1 text pieces, \\(\\theta\\)N1 of them make mistakes. It is easy to think that \\(\\theta\\)N1 /N = \\(\\theta\\) is equal to excess cross-entropy loss. However, Theorem 2 below shows those two are not equal. You can find the reconstruction of the proof in **Appendix 3.2.** Theorem 2 : if the average excess cross-entropy loss for the text-pieces is Œ¥ we conclude Y consists of at most 2Œ¥ fraction of text pieces.\nWe have theorem 3 below (you can find the proof in Appendix 3.3) guarantees that for most skills s, the model does not have significant error on the task associated with it. Note: theorem will give minimum guaranteed performance, actual performance could exceed this.\nTheorem 3: Let \\(\\alpha\\),\\(\\beta\\),\\(\\theta\\) \u003e 0, \\(\\beta\\)\u003e 1,\\(\\alpha\\beta\\) \u003c 1,\\(\\theta\\) \u003c 1 satisfy: \\[\\text{H}(\\theta) + k \\theta \\left( \\text{H}(\\beta \\alpha) - \\beta \\alpha \\log \\frac{1}{\\alpha} - (1-\\beta \\alpha) \\log \\left( \\frac{1}{1-\\alpha} \\right) \\right) = 0\\] where \\({H}(\\theta)\\) is Entropy defined as follows: \\[\\text{H}(x) = x \\log_2 \\frac{1}{x} + (1-x) \\log_2 \\frac{1}{1-x} \\tag{1}\\] We have the performance curve satisfying the theorem 3 as follows:\nFigure 8:see (Sanjeev et al. 2023). Performance Curves: The plot has theta = 0.1 and varies k = 2, 4, 8, 16. Higher values of k greatly improve performance.\nThe curve\\((k, \\theta)\\) above = set of \\((1-\\alpha,\\beta \\theta)\\) s.t. when excess c-e=\\(\\theta\\) then for \\(\\geq (1-\\alpha)\\) fraction of skills the model has error \\(\\leq \\beta \\theta\\) on the statistical task associated with that skill. For example, if \\(\\theta = 0.1\\), \\(\\alpha = 0.2\\), \\(\\beta = 3\\) \\(\\Rightarrow\\) For at least \\(1 - 0.2 = 0.8\\) fractions of skills, model answers incorrectly in at most 0.3 fraction of text pieces that used the skill. When we fix \\(\\theta\\), the emergence curves shift down noticeably (i.e., imply emergence of more skills) as we increase k. Note: please do not confuse k and k'. For simple skills, k means the number of simple skills that are required for generating text pieces. For complex skills, k also means the number of simple skills that are required for generating text pieces, but we have k' means the number of simple skills composed into complex skills. e.g., the perfromance curve shifts down if we increase the number of simple skills that are required for generating text pieces. But the performance curve shifts up(means more loss) for more complex skills(means bigger k\u0026rsquo;), see performance curve in 2.4.2.\nFigure 9: see (Sanjeev et al. 2023). Performance Curves: The plot has k = 8 and varies theta = 0.05, 0.1, 0.2.\nIf we fix k, when the model is scaled up, \\(\\theta\\) will go down, and the set Y containing erroneous answers on cloze questions will shrink. In terms of the emergence phenomenon, this corresponds to first signs of improvement of performance on tasks. 2.4.2 Emergence for Complex Skills We now derive emergence for tasks invloving complex (k\u0026rsquo;-tuples) skills. We have corollary 4 as follows:\nCorollary 4:In the same setting as Theorem 3 above,for integer \\(k' \\in [2, 1/\\theta]\\) the conclusion of that theorem holds also for \\(\\alpha, \\beta\\) pairs satisfying \\[\\text{H}(k'\\theta) + kk'\\theta \\left( \\text{H}(\\beta\\alpha) - \\beta\\alpha \\log \\frac{1}{\\alpha} - (1-\\beta\\alpha) \\log \\left( \\frac{1}{1-\\alpha} \\right) \\right) \u003c 0\\] Furthermore, if \\(\\text{H}(k'\\theta) \u003c k'\\text{H}(\\theta)\\) the emergence curve from this expression dominates that derived from Theorem 3 above. Figure 10: see (Sanjeev et al. 2023). Performance curve for t-tuples of skills for for theta = 0.05 and t = 1, 2, 4 respectively.\nBy the illustration in [(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936): From corollary 4, we can conclude that if \\(k'\\theta\\) is fixed, then we have same performance curve. It implies that the effect of reducing \\(\\theta\\) by a factor 2 has the effect of raising competence on 2k'-tuples to at least the same level as what it was on k'-tuples before reducing \\(\\theta\\). But how do we connect this with scaling law?\nBy the illustration in [(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936): Assume (for simplicity) a Chinchilla-like scaling law that 10x up-scaling leads to factor 2 reduction in excess entropy. By theorem 2, we know \\(\\theta\\) is proportional to excess cross entropy. So factor 2 reduction in \\(\\theta\\) corresponds factor 2 reduction in excess entrop. By the illustration in [(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936): If a model is considered to have reasonable performance on individual skill (1-tuple skills) at current scaling, then after further up-scaling of 10x(factor 2 reduction in \\(\\theta\\)) one would see similar reasonable performance on skill-pairs (2-tuples of skills), and scaling up by yet another 10x after that will yield similar reasonable performance on 4-tuples of skills, etc. Thus we have the result: the performance curve inferred by our method for k‚Äô-tuples of skills after scaling up is identical to the curve inferred for individual skills without scaling up. That means the competence of complex skills after scaling is not that samll, which means complex skills that were not seen during training must emerge.\nThe result is formalized as corollary 5 below: You can find the proof is in the paper.\nCorollary 5: When the model M1 with loss \\(\\delta\\) is scaled up (e.g., as per equation (3)) so that the new model M2 has loss \\(\\delta\\)/k', then the performance curve inferred by our method for k'-tuples of skills using M2 is identical to the curve inferred for individual skills on model M1. 2.4.3 Emergence analysis with general measure on text and skills Figure 11:Created by Xiaolan Liu. Skill graph where text pieces contain multiple skill clusters.\nMany skill clusters, not just one Each cluster has similar description as before, except skills of one cluster could be present in text in another cluster (e.g., ‚ÄùBasic English‚Äù needed in ‚ÄùLogic‚Äù)\nNothing much changes in theory, except now it only predicts emergence within a cluster if/when excess loss in that cluster goes down significantly.\nPart 3: Appendix 3.1 Theorem 1 If a model‚Äôs excess entropy at the ith place in text is œµ then there is a cloze question with binary answer such that the probability that the model answers it incorrectly is at most \\(\\sqrt{2 \\epsilon}\\). Proof:\nDefine \\( p_i \\) to be the human's probability for the \\((i+1)\\)-th word, \\( q_i = \\) model's probability for the \\((i+1)\\)-th word. Step 1:\nShow that the probability of \\( p_i \\) and \\( q_i \\) giving different answers is \\( \\max_A \\left| \\sum_{A} \\left( p_i(w) - q_i(w) \\right) \\right| \\), where \\( A \\) is any subset of words. Pf of Step 1: Note that the human's probability \\( p_i \\) on \\((i+1)\\)-th word is: \\[ p_i(w) = P(W_{i+1} = w) = \\begin{cases} 1, \u0026 \\text{if } w = w^* \\\\ 0, \u0026 \\text{otherwise} \\end{cases} \\] i.e., \\( p_i(w^*) = 1 \\), \\( P_i(w) = 0 \\) if \\( w \\neq w^* \\) So \\( p_i \\) will always answer \\( w^* \\). The answer for \\( q_i(\\cdot) \\) is \\( \\arg \\max_w q_i(w) \\), denoted as \\( w^{**} \\). So in order to give different answers between \\( p_i \\) and \\( q_i \\), we need \\( w^* \\neq w^{**} \\). Step 2:\nFrom Step 1: $$ P(p_i \\text{ and } q_i \\text{ give different answers}) = \\text{variation distance between } p_i \\text{ and } g_i $$\nUsing Pinsker\u0026rsquo;s Theorem: $$ \\leq \\sqrt{\\frac{1}{2} KL(p_i \\Vert q_i)} \\leq \\sqrt{\\frac{1}{2} \\epsilon} $$\nConstruct the cloze question as follows:\nDefine \\(A_{i+1}\\) as: $$ A_{i+1} = \\arg \\max_A \\left| \\sum_A (p_i(w) - q_i(w)) \\right| $$ Then, the cloze question is: Is the \\((i+1)\\)-th word \\(w^*\\) in \\((a)\\), \\(A_{i+1}\\), or \\((b)\\), in \\(A_{i+1}^c\\)? $$ \\text{Pr(answer to cloze question is wrong)} = P(w^* \\in A_{i+1} \\text{ but we answer } (b)) + P(w^* \\in A_{i+1}^c \\text{ but we answer } (a)) $$\n$$ = P(p_i\u0026rsquo;s \\text{ answer is in } A_{i+1}, q_i\u0026rsquo;s \\text{ answer is in } A_{i+1}^c) + P(p_i\u0026rsquo;s \\text{ answer is in } A_{i+1}^c, q_i\u0026rsquo;s \\text{ answer is in } A_{i+1}) $$\n$$ \\leq P(p_i\u0026rsquo;s \\text{ answer is different from } q_i) + P(p_i\u0026rsquo;s \\text{ answer is different from } q_i) $$\n$$ = 2 P(p_i\u0026rsquo;s \\text{ answer is different from } q_i) $$\nIf we denote \\(p_i's \\text{ answer is in } A_{i+1}, q_i's \\text{ answer is in } A_{i+1}^c\\) as \\(E\\), \\(p_i's \\text{ answer is in } A_{i+1}^c, q_i's \\text{ answer is in } A_{i+1}\\) as \\(F\\), \\(p_i's \\text{ answer is different from } q_i\\) as \\(G\\). Since both \\(E\\) and \\(F\\) are subsets of \\(G\\), if \\(p_i's\\) answer in \\(A_{i+1}\\), \\(q_i's\\) answer in \\(A_{i+1}^c\\). Obviously answers are different, $$ \\Rightarrow E \\subseteq G, \\ F \\subseteq G $$\nThen by Pinsker\u0026rsquo;s Theorem above:\n$$ \\Rightarrow 2 \\cdot \\sqrt{\\frac{1}{2} \\epsilon} = \\sqrt{2 \\epsilon} $$\n3.2 Theorem2 Theorem 2 : if the average excess cross-entropy loss for the text-pieces is Œ¥ we conclude Y consists of at most 2Œ¥ fraction of text pieces.\nProof:\nAssume there are \\( N \\) text pieces, each text piece has \\( Q_i \\) cloze questions. \\( \\delta \\) is the average ce loss. \\[ \\delta = \\frac{1}{N} \\sum_{i=1}^N \\text{cross entropy on text piece } i \\] Since the ce loss on the text piece = the ce loss of cloze questions on this text piece,\n\\[ \\Rightarrow \\delta= \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\text{cross entropy on cloze question } z_j^{(i)} \\] Since The average ce loss \\(\\geq\\) the loss in subset \\(Y\\). \\[ \\Rightarrow \\delta \\geq \\frac{1}{N} \\sum_{i \\in Y} \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\text{cross entropy on cloze question } z_j^{(i)} := A_i \\] Step 1: Prove that \\( A_i \\geq \\frac{1}{2} \\) Step 2: Prove that \\( \\theta \\leq 2\\delta \\) Pf of step 1:\n\\[ A_i = \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\text{cross entropy on cloze question } z_j^{(i)} \\] \\[ = \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\sum_w -p_i(w) \\log q_i(w) \\] where \\(w\\) is all of the possible answers. \\[ = \\frac{1}{Q_i} \\left( \\sum_{j=1}^{Q_i} \\sum_w -p_i(w) \\log q_i(w) \\text{ (model is correct on } z_j^{(i)}) + \\sum_{j=1}^{Q_i} \\sum_w -p_i(w) \\log q_i(w) \\text{ (model is wrong on } z_j^{(i)})\\right) \\] \\[ \\geq \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\sum_w -p_i(w) \\log q_i(w)\\text{ (model is wrong on } z_j^{(i)}) \\] Assume \\(w*\\) is the correct answer. \\(\\Rightarrow p_i(w^*) = 1, p_i(w) = 0 \\text{ if } w \\ne w^*\\) \\[ \\Rightarrow \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} -\\log q_i(w^*) \\text{ (model is wrong on } z_j^{(i)}) \\] \\(\\text{Since model is wrong on } z_j^{(i)}\\) \\[ \\Rightarrow -\\log q_i(w^*) \\leq 1 \\quad (\\text{otherwise model will be correct}) \\] \\[ \\Rightarrow -\\log q_i(w^*) \\geq 1 \\] \\[ \\Rightarrow A_i = \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\left( -\\log q_i(w^*) \\right)\\text{(model is wrong on } z_j^{(i)}) \\] \\[ \\geq \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} 1 \\text{(model is wrong on } z_j^{(i)}) \\] Define set \\( S_i = \\{z_j^{(i)} : \\text{model is wrong on } z_j^{(i)}\\} \\) Since \\(Y\\) is the set of text pieces with each has at least half of wrong cloze questions on it. \\[ \\text{by definition } \\Rightarrow |S_i| \\geq \\frac{1}{2} |Q_i| \\] \\[ \\Rightarrow A_i \\geq \\frac{1}{Q_i} |S_i| \\geq \\frac{1}{2} \\] Pf of step 2:\nSince \\( \\delta \\geq \\frac{1}{N} \\sum_{i \\in Y} A_i \\) \\[ \\geq \\frac{1}{N} \\sum_{i \\in Y} \\frac{1}{2} \\quad (\\text{from step 1, } A_i \\geq \\frac{1}{2}) \\] \\[ = \\frac{1}{2N} |Y| \\] \\[ = \\frac{1}{2N} \\cdot \\theta N \\] \\[ \\Rightarrow \\theta \\leq 2 \\delta \\] 3.3 Theorem 3 Theorem 3: Let \\(\\alpha\\),\\(\\beta\\),\\(\\theta\\) \u003e 0, \\(\\beta\\)\u003e 1,\\(\\alpha\\beta\\) \u003c 1,\\(\\theta\\) \u003c 1 satisfy: \\[\\text{H}(\\theta) + k \\theta \\left( \\text{H}(\\beta \\alpha) - \\beta \\alpha \\log \\frac{1}{\\alpha} - (1-\\beta \\alpha) \\log \\left( \\frac{1}{1-\\alpha} \\right) \\right) = 0\\] where \\({H}(\\theta)\\) is Entropy defined as follows: \\[\\text{H}(x) = x \\log_2 \\frac{1}{x} + (1-x) \\log_2 \\frac{1}{1-x} \\tag{1}\\] Proof:\nWe say model makes a mistake in a text piece if it fails to answer \u0026gt;= half of its cloze questions.\nSuppose \\(V1\\) is the set for text pieces and \\(V2\\) is the set for skills. Suppose \\(Y \\subseteq V1\\) of size \\(\\theta N1\\) is the subset of text pieces where model makes mistakes. We need to show that thare are at least \\((1-\\alpha)\\) fraction of vertices in \\(V2\\) each of which has at most \\(\\beta \\theta D\\) edges going to Y, where \\(D=\\frac{kN1}{N2}\\). Define a skill to be \"good\" if it has at most \\(\\beta\\theta D\\) edges going to Y, and \"bad\" if it has more than \\(\\beta\\theta D\\) edges going to Y. So we need to show that there are at least \\((1 - \\alpha) N2\\) good skills, or equivalently, show that there are at most \\(\\alpha N2\\) fraction bad skills. Define \\(Z \\subseteq V2, |Z2| \\leq \\alpha N2\\) be the subset of \\(V2\\) such that \\(Z\\) has at least \\(\\alpha \\beta \\theta kN1\\) edges to \\(Y\\). So \\(Z\\) is potentially the set for all \"bad\" skills. (Since \\((\\beta \\theta D) * (\\alpha N2) = \\alpha \\beta \\theta kN1)\\). If we can show that the expected number of such \\(Z\\)s is at most 1, then we can conclude the theorem because we have at most 1 such bad subset of skills, each has at most \\(\\alpha N2\\) bad skills, making it total at most \\(\\alpha N2\\) bad skills in \\(V2\\). By some combinatorics we conclude that the expected number of such \\(Z\\)s can be upper bounded by $$ N1N2\\binom{N2}{\\alpha N2}\\times \\binom{N1}{\\theta N1} \\times \\binom{k\\theta N_1}{\\beta \\alpha k \\theta N_1} \\times\\alpha^{\\beta\\alpha\\theta kN_1}\\times (1-\\alpha)^{(1-\\beta\\alpha)\\theta k N_1} $$ Note that the latter three terms is the probability in binomial distribution that among total \\(k\\theta N_1\\)outgoing edges from \\(Y\\), the probability of \\(\\beta \\alpha k\\theta N_1\\) of them connect to the vertices in Z. We need to show that the above formula can be upper bounded by 1, which can be shown by showing that the log of above is negative. Part 4: reference (Sanjeev et al. 2023)\n(Anderson, 1972)\n(Wei et al. 2022)\n(Schaeffer et al. 2023)\n(Lu et al. 2023)\n","permalink":"https://XIAOLAN-design.github.io/posts/emergence/","summary":"This Blog is devided into 2 parts. First part is the review of discussion of Emergence. You can just skip part 2 if you are not interested in theoretical staff. Second part illustrates the theory of emergence of complex skills in language model in a way people can understand easily, which is based on (Sanjeev et al. 2023) and the modified version of the paper Prof Arora (the author of the paper) sent me.","title":"Emergence of Complex Skills in Language Model"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;‚Äî\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n‚Äî Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested Unordered list Fruit Apple Orange Banana Dairy Milk Cheese Nested Ordered list Fruit Apple Orange Banana Dairy Milk Cheese Third item Sub One Sub Two Other Elements ‚Äî abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://XIAOLAN-design.github.io/posts/markdown-syntax/","summary":"\u003cp\u003eThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\u003c/p\u003e","title":"Markdown Syntax Guide"},{"content":"Inline Code This is Inline Code\nOnly pre This is pre text Code block with backticks \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with backticks and language specified \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with backticks and language specified with line numbers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with line numbers and highlighted lines PaperMod supports linenos=true or linenos=table 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; With linenos=inline line might not get highlighted properly. This issue is fixed with 045c084 1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6 \u0026lt;meta 7 name=\u0026#34;description\u0026#34; 8 content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; 9 /\u0026gt; 10 \u0026lt;/head\u0026gt; 11 \u0026lt;body\u0026gt; 12 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 13 \u0026lt;/body\u0026gt; 14\u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Github Gist ","permalink":"https://XIAOLAN-design.github.io/posts/code_syntax/","summary":"Sample article showcasing basic code syntax and formatting for HTML elements.","title":"Code Syntax Guide"},{"content":"Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\nFigure Shortcode (PaperMod enhanced) Photo by Aditya Telange on Unsplash\nYouTube Twitter Shortcode PaperMod is now the most starred @GoHugoIO theme on #GitHub ! ‚ú®\nHere\u0026#39;s what it offers:\n- Simple, minimal \u0026amp; clean design\n- Light/Dark mode\n- Fuzzy search for content\n- Good page-speed insights\nand much more...\nHuge thanks to all supportersüôèhttps://t.co/YAEd2cfrrn\n\u0026mdash; Aditya Telange üî≠ (@adityatelange) November 14, 2023 Vimeo Shortcode ","permalink":"https://XIAOLAN-design.github.io/posts/rich-content/","summary":"\u003cp\u003eHugo ships with several \u003ca href=\"https://gohugo.io/content-management/shortcodes/#use-hugos-built-in-shortcodes\"\u003eBuilt-in Shortcodes\u003c/a\u003e for rich content, along with a \u003ca href=\"https://gohugo.io/about/hugo-and-gdpr/\"\u003ePrivacy Config\u003c/a\u003e and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\u003c/p\u003e","title":"Rich Content and Shortcodes"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates (extend_head.html) like so: refer ISSUE #236 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: \\(\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887‚Ä¶\\) Block math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","permalink":"https://XIAOLAN-design.github.io/posts/math-typesetting/","summary":"\u003cp\u003eMathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\u003c/p\u003e","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\nüôà :see_no_evil: üôâ :hear_no_evil: üôä :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","permalink":"https://XIAOLAN-design.github.io/posts/emoji-support/","summary":"\u003cp\u003eEmoji can be enabled in a Hugo project in a number of ways.\u003c/p\u003e","title":"Emoji Support"}]